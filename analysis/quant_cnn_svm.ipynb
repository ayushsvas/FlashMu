{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION SCRIPT FOR SMVs and CNNs\n",
    "The way this works is that you place all your prediction files in a folder/list. Be it for synthetic holograms or real. \n",
    "GT data is available in a .pkl file for synthetic and .txt files for real holograms. The script takes care of these. \n",
    "Make sure you put your SVM/CNN predictions in the same folder. If they're not, combine them in a single list. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from utils import f1, recall, precision, dice_score\n",
    "import mat73\n",
    "import glob\n",
    "from patchify import patchify, unpatchify\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gkern(l=5, sig=1.):\n",
    "    \"\"\"\\\n",
    "    creates gaussian kernel with side length `l` and a sigma of `sig`\n",
    "    \"\"\"\n",
    "    ax = np.linspace(-(l - 1) / 2., (l - 1) / 2., l)\n",
    "    gauss = np.exp(-0.5 * np.square(ax) / np.square(sig))\n",
    "    kernel = np.outer(gauss, gauss)\n",
    "    return kernel\n",
    "\n",
    "def peak_local_max(input, threshold_abs=1, min_distance=1):\n",
    "    '''\n",
    "    Returns a binary map where maxima positions are true.\n",
    "\n",
    "        Parameters:\n",
    "            input (pytorch tensor): image-like pytorch tensor of dimension [batch_size, channels, width, height], where each image will be processed individually\n",
    "            threshold_abs (float): local maxima below will be dropped\n",
    "            min_distance (int): min distance (in pixels) between two maxima detections.\n",
    "        Returns\n",
    "            pytorch tensor of same shape as input\n",
    "    '''\n",
    "    max_filtered=nn.functional.max_pool2d(input, kernel_size=2*min_distance+1, stride=1, padding=min_distance)\n",
    "    maxima = torch.eq(max_filtered, input)\n",
    "    return maxima * (input >= threshold_abs)\n",
    "\n",
    "def mask_maker(X, Y, Z, R, N_x = 512, N_y = 512, kernel_size = 5):\n",
    "    X = np.round(np.array(X)).astype(int)\n",
    "    Y = np.round(np.array(Y)).astype(int)\n",
    "    \n",
    "    Z = (np.array(Z))\n",
    "    R = (np.array(R))\n",
    "    \n",
    "    \n",
    "    gk = gkern(l = kernel_size, sig = 1)\n",
    "    mask  = np.zeros((3, N_y + kernel_size + 2, N_x + kernel_size + 2))\n",
    "    \n",
    "    for (x,y,z,r) in zip(X,Y,Z,R):\n",
    "\n",
    "        mask[0,(kernel_size//2+y-kernel_size//2):(kernel_size//2+y+kernel_size//2+1),(kernel_size//2+x-kernel_size//2):(kernel_size//2+x+kernel_size//2+1)] = gk # Use this for synthetic \n",
    "        mask[1,(kernel_size//2+y-kernel_size//2):(kernel_size//2+y+kernel_size//2+1),(kernel_size//2+x-kernel_size//2):(kernel_size//2+x+kernel_size//2+1)] = z\n",
    "        mask[2,(kernel_size//2+y-kernel_size//2):(kernel_size//2+y+kernel_size//2+1),(kernel_size//2+x-kernel_size//2):(kernel_size//2+x+kernel_size//2+1)] = r\n",
    "\n",
    "\n",
    "    return mask[:,1+kernel_size//2:N_y+1+kernel_size//2,1+kernel_size//2:N_x+1+kernel_size//2]\n",
    "\n",
    "\n",
    "\n",
    "def make_real_gt_mask(pth_to_real_gt, Z_locs, units, file_index, hologram_size, edge_crop_dist, ds_factor, hitbox):\n",
    "\n",
    "    gt_files = glob.glob(pth_to_real_gt+\"*.txt\")\n",
    "    dx, dz, dr = units\n",
    "\n",
    "    num_holograms = 1 if file_index is not None else len(gt_files)\n",
    "\n",
    "    holo_size = (hologram_size-2*edge_crop_dist)\n",
    "    xy_slices = (slice(3), slice(edge_crop_dist//ds_factor, (hologram_size-edge_crop_dist)//ds_factor),\n",
    "              slice(edge_crop_dist//ds_factor, (hologram_size-edge_crop_dist)//ds_factor))\n",
    "    \n",
    "    masks = np.zeros((num_holograms, 3 ,holo_size, holo_size))\n",
    "\n",
    "    for j, gt_file in enumerate(gt_files):\n",
    "        data = np.genfromtxt(gt_file)\n",
    "        x = data[:,0]\n",
    "        y = data[:,1]\n",
    "        d = data[:,2]*dr\n",
    "        z = np.ones(x.shape[0])*Z_locs[j]\n",
    "\n",
    "        xmin = 0\n",
    "        ymin = 0\n",
    "        xmax = hologram_size\n",
    "        ymax = hologram_size\n",
    "        \n",
    "\n",
    "        xx = x[(x>xmin)*(y>ymin)*(x<xmax)*(y<ymax)]\n",
    "        yy = y[(x>xmin)*(y>ymin)*(x<xmax)*(y<ymax)]\n",
    "        zz = z[(x>xmin)*(y>ymin)*(x<xmax)*(y<ymax)]\n",
    "        dd = d[(x>xmin)*(y>ymin)*(x<xmax)*(y<ymax)]\n",
    "        \n",
    "        tgt = mask_maker(xx/ds_factor, yy/ds_factor, zz, dd, xmax//ds_factor, ymax//ds_factor, kernel_size = hitbox)\n",
    "        # print(tgt.shape)\n",
    "\n",
    "        true_mask_xy = np.flipud(tgt[0]) # Mask for standard method on 5120x5120\n",
    "        true_mask_z = np.flipud(tgt[1])\n",
    "        true_mask_r = np.flipud(tgt[2])\n",
    "        true_mask = np.concatenate((true_mask_xy[np.newaxis,:,:], true_mask_z[np.newaxis,:,:], true_mask_r[np.newaxis,:,:]), axis = 0)\n",
    "\n",
    "        masks[j] = true_mask[xy_slices]\n",
    "\n",
    "    return masks\n",
    "\n",
    "def make_synthetic_gt_mask(pkl_file, num_holos, units, holo_size, crop_size, step_size, ds_factor, kernel_size):\n",
    "    dx, dz, dr = units\n",
    "    dx = dx*ds_factor\n",
    "    holo_size = holo_size//ds_factor\n",
    "    patch_size = crop_size//ds_factor\n",
    "    step = step_size//ds_factor\n",
    "    num_patches_per_holo = (holo_size//patch_size)**2\n",
    "    store_masks = np.zeros((num_holos*num_patches_per_holo, 3, patch_size, patch_size), dtype = np.float16)\n",
    "    \n",
    "    msk_count = 0\n",
    "    for i in trange(num_holos):\n",
    "        mask = mask_maker(pkl_file['x'][i]/dx+holo_size//2, pkl_file['y'][i]/dx+holo_size//2,\n",
    "                          pkl_file['z'][i]/dz,pkl_file['r'][i]/dr, holo_size, holo_size, kernel_size)\n",
    "        patched_mask_xy = patchify(mask[0], patch_size=(patch_size), step = step)\n",
    "        patched_mask_xy = np.reshape(patched_mask_xy, (patched_mask_xy.shape[0]*patched_mask_xy.shape[1], patched_mask_xy.shape[2], patched_mask_xy.shape[3]))\n",
    "\n",
    "\n",
    "        patched_mask_z = patchify(mask[1], patch_size=(patch_size), step = step)\n",
    "        patched_mask_z = np.reshape(patched_mask_z, (patched_mask_z.shape[0]*patched_mask_z.shape[1], patched_mask_z.shape[2], patched_mask_z.shape[3]))\n",
    "\n",
    "\n",
    "        patched_mask_r = patchify(mask[2], patch_size=(patch_size), step = step)\n",
    "        patched_mask_r = np.reshape(patched_mask_r, (patched_mask_r.shape[0]*patched_mask_r.shape[1], patched_mask_r.shape[2], patched_mask_r.shape[3]))\n",
    "        \n",
    "        store_masks[msk_count:msk_count+patched_mask_xy.shape[0],0] = patched_mask_xy\n",
    "        store_masks[msk_count:msk_count+patched_mask_z.shape[0],1] = patched_mask_z\n",
    "        store_masks[msk_count:msk_count+patched_mask_r.shape[0],2] = 2*patched_mask_r # radius in the ground truth\n",
    "\n",
    "        msk_count += patched_mask_xy.shape[0]\n",
    "    \n",
    "    return store_masks\n",
    "\n",
    "def make_cnn_prediction_masks(pth_to_preds, cutoff, hologram_size, units, ds_factor, kernel_size,):\n",
    "    dx, dz, dr = units\n",
    "    dx = dx*ds_factor\n",
    "    holo_size = hologram_size//ds_factor\n",
    "\n",
    "\n",
    "    predictions_list = sorted(glob.glob(pth_to_preds+\"*.txt\"))\n",
    "\n",
    "    masks = np.zeros((len(predictions_list, 3, holo_size, holo_size)), dtype=np.float16)\n",
    "\n",
    "    for i,file in enumerate(predictions_list):\n",
    "        data = np.genfromtxt(file)\n",
    "        data = data[data[:,-1]>=cutoff]\n",
    "        x = data[:,0]/(dx) + holo_size//2\n",
    "        y = data[:,1]/(dx) + holo_size//2\n",
    "        z = data[:,2]/dz \n",
    "        d = data[:,3]/dr \n",
    "        \n",
    "        masks[i] = mask_maker(x, y, z, d, holo_size, holo_size, kernel_size)\n",
    "    \n",
    "    return masks \n",
    "\n",
    "def make_svm_prediction_masks(pth_to_preds, cutoff, hologram_size, units, ds_factor, kernel_size,):\n",
    "    dx, dz, dr = units\n",
    "    dx = dx*ds_factor\n",
    "    holo_size = hologram_size//ds_factor\n",
    "\n",
    "    predictions_list = sorted(glob.glob(pth_to_preds+\"*.mat\"))\n",
    "\n",
    "    masks = np.zeros((len(predictions_list, 3, holo_size, holo_size)), dtype=np.float16)\n",
    "\n",
    "    for i,file in enumerate(predictions_list):\n",
    "        data = mat73.loadmat(file)\n",
    "        x = data['metrics'][:,100]/dx + holo_size//2\n",
    "        y = data['metrics'][:,101]/dx + holo_size//2\n",
    "        z = data['metrics'][:,105]/dz\n",
    "        d = np.sqrt(4/(np.pi)*data['metrics'][:,1])/dr\n",
    "\n",
    "        masks[i] = mask_maker(x, y, z, d, holo_size, holo_size, kernel_size)\n",
    "    \n",
    "    return masks \n",
    "\n",
    "def get_bad_particles_counts(pred_data, true_data, hits, constraints):\n",
    "    z_values_detected_wrt_gt = true_data[:,1].unsqueeze(1)[hits]\n",
    "    s_values_detected_wrt_gt = true_data[:,2].unsqueeze(1)[hits]\n",
    "    z_values_detected_wrt_pred = pred_data[:,1].unsqueeze(1)[hits]\n",
    "    s_values_detected_wrt_pred = pred_data[:,2].unsqueeze(1)[hits]\n",
    "    ez = np.abs(z_values_detected_wrt_pred-z_values_detected_wrt_gt)\n",
    "    er = np.abs(s_values_detected_wrt_pred-s_values_detected_wrt_gt)\n",
    "    \n",
    "    outliers = 0\n",
    "    out_of_scope = 0 # This variable is for removing out of scope particles (<min_r with respect to gt) from the intersection and predictions.\n",
    "    \n",
    "    min_r = 6.0 # put diameter here\n",
    "    max_r = 75.0\n",
    "    min_z ,max_z, min_r, max_r, allowed_ez = constraints\n",
    "    allowed_ez = 10\n",
    "    for zz, zz_pred, ss, ss_pred, ezz, err in zip(z_values_detected_wrt_gt, z_values_detected_wrt_pred, s_values_detected_wrt_gt, s_values_detected_wrt_pred, ez, er):\n",
    "        if ss < min_r or ss > max_r:  # take care of the inequalities here. \n",
    "            out_of_scope += 1\n",
    "            continue\n",
    "\n",
    "        if ezz >allowed_ez: \n",
    "            outliers += 1\n",
    "            continue\n",
    "    return out_of_scope, outliers\n",
    "\n",
    "def get_z_and_size(pred_masks, true_masks, best_cutoff, min_distance, hit_box_size_param, constraints):\n",
    "    z_det, z_pred, d_det, d_pred ,ez_det, ed_det = [], [], [], [], [], []\n",
    "    min_z ,max_z, min_r, max_r, allowed_ez = constraints\n",
    "    for pred_data, true_data in zip(pred_masks, true_masks):\n",
    "        predicted_particles = peak_local_max(pred_data[:,0].unsqueeze(1), threshold_abs=best_cutoff, min_distance=min_distance).float()\n",
    "        hits = peak_local_max(predicted_particles*((true_data[0,0]>=hit_box_size_param).float()).unsqueeze(0), threshold_abs=1, min_distance=min_distance)\n",
    "\n",
    "        z_values_detected_wrt_gt = true_data[:,1].unsqueeze(1)[hits]\n",
    "        s_values_detected_wrt_gt = true_data[:,2].unsqueeze(1)[hits]\n",
    "        z_values_detected_wrt_pred = pred_data[:,1].unsqueeze(1)[hits]\n",
    "        s_values_detected_wrt_pred = pred_data[:,2].unsqueeze(1)[hits]\n",
    "        ez = np.abs(z_values_detected_wrt_pred-z_values_detected_wrt_gt)\n",
    "        er = np.abs(s_values_detected_wrt_pred-s_values_detected_wrt_gt)\n",
    "        \n",
    "        outlier = 0\n",
    "        out_of_scope = 0 # This variable is for removing out of scope particles (<min_r with respect to gt) from the intersection and predictions.\n",
    "    \n",
    "        for zz, zz_pred, ss, ss_pred, ezz, err in zip(z_values_detected_wrt_gt, z_values_detected_wrt_pred, s_values_detected_wrt_gt, s_values_detected_wrt_pred, ez, er):\n",
    "            if ss < min_r or ss > max_r:  # take care of the inequalities here. \n",
    "                out_of_scope += 1\n",
    "                continue\n",
    "\n",
    "            if ezz >allowed_ez: \n",
    "                outlier += 1\n",
    "                continue\n",
    "\n",
    "            z_det.append(float(zz))\n",
    "            z_pred.append(float(zz_pred))\n",
    "            d_det.append(float(ss))\n",
    "            d_pred.append(float(ss_pred))\n",
    "        \n",
    "    return z_det, z_pred, d_det, d_pred\n",
    "\n",
    "def get_hists(true_masks, gt_thresh, min_distance, constraints):\n",
    "    hist_true_r = []\n",
    "    hist_true_z = []\n",
    "    min_z ,max_z, min_r, max_r, ez_allowed = constraints\n",
    "    for true_data in true_masks:\n",
    "            peaks_in_true_data = peak_local_max(true_data[0,0].unsqueeze(0), gt_thresh, min_distance).squeeze(0)\n",
    "            depths_in_true_data = true_data[0,1][peaks_in_true_data]\n",
    "            sizes_in_true_data = true_data[0,2][peaks_in_true_data]\n",
    "            for depth, size in zip(depths_in_true_data,sizes_in_true_data):\n",
    "                if size < min_r and size > max_r:\n",
    "                    continue\n",
    "                hist_true_r.append(size)\n",
    "                hist_true_z.append(depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall_f1(pred_masks, true_masks, constraints, hitbox, cutoff_vals):\n",
    "\n",
    "    min_z ,max_z, min_r, max_r, ez_allowed = constraints\n",
    "    hit_box_size_param = gkern(hitbox, 1).min() \n",
    "    gt_thresh = 0.8\n",
    "    min_distance = hitbox//2\n",
    "    num_samples = pred_masks.shape[0]\n",
    "\n",
    "    for cutoff in cutoff_vals:\n",
    "        P = []\n",
    "        R = []\n",
    "        F1 = []\n",
    "\n",
    "        pr = 0\n",
    "        rc = 0\n",
    "        f1sc = 0\n",
    "        for pred_data, true_data in zip(pred_masks, true_masks):\n",
    "\n",
    "            predicted_particles = peak_local_max(pred_data[:,0].unsqueeze(1), threshold_abs=gt_thresh, min_distance=min_distance).float()\n",
    "            hits = peak_local_max(predicted_particles*((true_data[0,0]>=hit_box_size_param).float()).unsqueeze(0), threshold_abs=1, min_distance=min_distance)\n",
    "\n",
    "            out_of_scope, outliers = get_bad_particles_counts(pred_data, true_data, hits, constraints)\n",
    "            \n",
    "\n",
    "            hits_ = (hits.sum()-out_of_scope)-outliers\n",
    "\n",
    "            fp = (predicted_particles.sum()-out_of_scope) - (hits_)\n",
    "            \n",
    "            out_of_scope_for_gt = ((((true_data[:,2].unsqueeze(1))[peak_local_max(true_data[:,0].unsqueeze(1), gt_thresh, 1)]< min_r)\n",
    "                                    +((true_data[:,2].unsqueeze(1))[peak_local_max(true_data[:,0].unsqueeze(1), gt_thresh, 1)]> max_r)).float()).sum() # This variable is for removing all the particles <min_r from gt. > 0.9 takes care when sum is 2 (True + True), it converts it to 1.\n",
    "            fn = (peak_local_max(true_data[:,0].unsqueeze(1), 0.8, 1).sum()-out_of_scope_for_gt) - (hits_) # Have to add outlier to hits because calculation happens in xy space for peak_local_max(true_data) and the outlier is added to false positive\n",
    "\n",
    "            if hits_ > fn+hits_:\n",
    "                extra_hits -= (fn - hits_)\n",
    "                hits_ -= extra_hits\n",
    "                fp += extra_hits\n",
    "\n",
    "\n",
    "            pr += precision(hits_,fp)\n",
    "            rc += recall(hits_,fn)\n",
    "            f1sc += f1(pr,rc)\n",
    "\n",
    "        P = float(pr/num_samples)\n",
    "        R = float(rc/num_samples)\n",
    "        F1 = float(f1sc/num_samples)\n",
    "    \n",
    "    return P, R, F1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the grount truth (.pkl file)\n",
    "pth_to_syn_gt = ''\n",
    "num_synthetic_test_holos = 1000\n",
    "dr = 1 # in the gt only xy are in m, size and z are in Âµm and mm.\n",
    "dz = 1\n",
    "dx = 3e-6\n",
    "hologram_size = 4096\n",
    "crop_size = 1536\n",
    "ds_factor = 4\n",
    "hitbox = 21//ds_factor\n",
    "tgt_pkl = np.load(pth_to_syn_gt, allow_pickle=True)\n",
    "tgt_masks = make_synthetic_gt_mask(tgt_pkl, num_synthetic_test_holos, (dx, dz, dr) ,holo_size = hologram_size, \n",
    "                          crop_size = crop_size, step_size = crop_size, ds_factor = ds_factor, kernel_size=hitbox)\n",
    "\n",
    "\n",
    "\n",
    "pth_to_real_gt = ''\n",
    "dr = 3 #size is given in pixels for some reason in CloudTarget\n",
    "dz = 1\n",
    "dx = 1 # (x,y) also in pixels \n",
    "units = (dx, dz, dr)\n",
    "hologram_size = 5120\n",
    "ds_factor = 4\n",
    "hitbox = 21//ds_factor\n",
    "edge_crop_dist = 512 #512, #256\n",
    "Zs = [50,99,167,192,75]\n",
    "tgt_masks = make_real_gt_mask(pth_to_real_gt, Zs, units, None, hologram_size, edge_crop_dist, ds_factor, hitbox)\n",
    "tgt = torch.from_numpy(tgt_masks)\n",
    "\n",
    "# load the predictions from CNNs (.txt files )\n",
    "# cnn preds are in m\n",
    "pth_to_cnn_preds = ''\n",
    "dr = 1e-6\n",
    "dz = 1e-3\n",
    "dx = 3e-6\n",
    "hologram_size = 1536 # 5120\n",
    "ds_factor = 4\n",
    "pred_kernel_size = 3\n",
    "\n",
    "\n",
    "cutoff = 0.69 # random value for svm\n",
    "cutoff_vals = [cutoff]\n",
    "cutoff_vals = np.arange(100)/100 # for cnn\n",
    "\n",
    "# Constraints\n",
    "min_z = 5\n",
    "max_z = 200\n",
    "min_r = 6.0 # this is the real parameter setting precision recall with respect to r\n",
    "max_r = 100\n",
    "ez_allowed = 10\n",
    "constraints = (min_z, max_z, min_r, max_r)\n",
    "hit_box_size_param = gkern(hitbox, 1).min()\n",
    "gt_thresh = 0.8\n",
    "min_distance = hitbox//2\n",
    "\n",
    "\n",
    "xy_slices = (slice(None), slice(None), slice(None), slice(None)) # for synthetic \n",
    "xy_slices = (slice(None), slice(3), slice(edge_crop_dist//ds_factor, (hologram_size-edge_crop_dist)//ds_factor),\n",
    "            slice(edge_crop_dist//ds_factor, (hologram_size-edge_crop_dist)//ds_factor)) # for real \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate stats\n",
    "prec, rec, f1_score = [], [], []\n",
    "for cutoff in cutoff_vals:\n",
    "    pred_masks = make_cnn_prediction_masks(pth_to_cnn_preds, cutoff, hologram_size, (dx, dz, dr), ds_factor, pred_kernel_size)\n",
    "    # pred_masks = make_svm_prediction_masks(pth_to_cnn_preds, cutoff, hologram_size, (dx, dz, dr), ds_factor, pred_kernel_size)\n",
    "\n",
    "    # convert to torch\n",
    "    preds = torch.from_numpy(pred_masks)[xy_slices]\n",
    "\n",
    "    P, R, F1 = get_precision_recall_f1(tgt, preds, constraints, hitbox)\n",
    "    prec.append(P)\n",
    "    rec.append(R)\n",
    "    f1_score.append(F1)\n",
    "\n",
    "\n",
    "best_cutoff_idx = np.argmax(F1)\n",
    "best_cutoff = cutoff_vals[best_cutoff_idx]\n",
    "best_f1 = F1[best_cutoff_idx]\n",
    "rec_at_best_f1 = R[best_cutoff_idx]\n",
    "prec_at_best_f1 = P[best_cutoff_idx]\n",
    "\n",
    "z_det, z_pred, d_det, d_pred = get_z_and_size(preds, tgt_masks, best_cutoff, min_distance, hit_box_size_param, constraints)\n",
    "    \n",
    "hist_true_z, hist_true_r = get_hists(tgt_masks, gt_thresh, min_distance, constraints)\n",
    "\n",
    "prediction_dict = {\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"F1\": f1_score,\n",
    "        \"best_F1\":  float(best_f1),\n",
    "        \"precision_at_best_f1\": float(prec_at_best_f1),\n",
    "        \"recall_at_best_f1\": float(rec_at_best_f1),\n",
    "        \"best_cutoff\": float(best_cutoff),\n",
    "        \"best_cutoff_idx\": best_cutoff_idx,\n",
    "        \"z_detected\": np.array(z_det),\n",
    "        \"z_predicted\": np.array(z_pred),\n",
    "        \"d_detected\": np.array(d_det),\n",
    "        \"d_predicted\": np.array(d_pred),\n",
    "        \"z_all_in_gt\": np.array(hist_true_z),\n",
    "        \"d_all_in_gt\": np.array(hist_true_r)\n",
    "    }\n",
    "\n",
    "# qualitatively check for overlap\n",
    "plt.imshow(tgt_masks[0,0])\n",
    "plt.imshow(pred_masks[0,0], alpha = 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions \n",
    "with open(\"\"+\"CNN_preds.pkl\", mode = 'wb+') as f:\n",
    "    pickle.dump(prediction_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tryhard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
